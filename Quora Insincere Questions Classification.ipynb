{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Insincere Questions Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will be predicting whether a question asked on Quora is sincere or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An insincere question is defined as a question intended to make a statement rather than look for helpful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tensorflow.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tensorflow.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tensorflow.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('C:\\\\Users\\\\maryam\\\\Desktop\\\\Python Program\\\\Deep Learning\\\\2\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\maryam\\\\Desktop\\\\Python Program\\\\Deep Learning\\\\2\\\\test.csv')\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps are as follows:\n",
    "\n",
    "1) Split the training dataset into train and val sample. Cross validation is a time consuming process and so let us do simple train val split.\n",
    "\n",
    "2) Fill up the missing values in the text column with 'na'\n",
    "\n",
    "3) Tokenize the text column and convert them to vector sequences\n",
    "\n",
    "4) Pad the sequence as needed - if the number of words in the text is greater than 'max_len' trunacate them to 'max_len' or if the number of words in the text is lesser than 'max_len' add zeros for remaining values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split to train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill up the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>651064</th>\n",
       "      <td>7f8590ef60e30b4344fd</td>\n",
       "      <td>What have been the best exhibits at the Museo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294259</th>\n",
       "      <td>fda9538a2e0a5b2dfc3c</td>\n",
       "      <td>How can I rotate batch image files?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "651064   7f8590ef60e30b4344fd   \n",
       "1294259  fda9538a2e0a5b2dfc3c   \n",
       "\n",
       "                                             question_text  target  \n",
       "651064   What have been the best exhibits at the Museo ...       0  \n",
       "1294259                How can I rotate batch image files?       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What have been the best exhibits at the Museo del Prado in Madrid?',\n",
       "       'How can I rotate batch image files?',\n",
       "       'Which is the best cable operator in Thane west area?', ...,\n",
       "       'Do we need a prescription for cough syrup in Egypt?',\n",
       "       'What are the best and worst aspects of being a travel agent?',\n",
       "       'Who was a person you met who gave a very good vibe/ good-spirit that you remained friends with through life?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1175509, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Pretrained Embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that we are done with all the necessary preprocessing steps, we can first train a Bidirectional GRU model. \n",
    "We will not use any pre-trained word embeddings for this model and the embeddings will be learnt from scratch. \n",
    "Please check out the model summary for the details of the layers used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from keras.backend import set_session\n",
    "\n",
    "config_proto = tensorflow.compat.v1.ConfigProto()\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
    "session = tensorflow.compat.v1.Session(config=config_proto)\n",
    "tensorflow.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow' from 'C:\\\\Users\\\\maryam\\\\Anaconda3\\\\envs\\\\tf_gpu\\\\lib\\\\site-packages\\\\tensorflow\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,241\n",
      "Trainable params: 15,142,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using train sample and monitor the metric on the valid sample. This is just a sample model running for 2 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maryam\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 6355s 5ms/step - loss: 0.1227 - accuracy: 0.9536 - val_loss: 0.1069 - val_accuracy: 0.9562\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 8543s 7ms/step - loss: 0.0980 - accuracy: 0.9610 - val_loss: 0.1073 - val_accuracy: 0.9567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10e02bf1e88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get the validation sample predictions and also get the best threshold for F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 188s 1ms/step\n",
      "F1 score at threshold 0.1 is 0.548190504852908\n",
      "F1 score at threshold 0.11 is 0.5582149474984558\n",
      "F1 score at threshold 0.12 is 0.5674554576699719\n",
      "F1 score at threshold 0.13 is 0.5762164344841512\n",
      "F1 score at threshold 0.14 is 0.583508295253386\n",
      "F1 score at threshold 0.15 is 0.5900125891733109\n",
      "F1 score at threshold 0.16 is 0.5964192624877152\n",
      "F1 score at threshold 0.17 is 0.601500498720673\n",
      "F1 score at threshold 0.18 is 0.607649599012955\n",
      "F1 score at threshold 0.19 is 0.6116604769788357\n",
      "F1 score at threshold 0.2 is 0.6158245948522403\n",
      "F1 score at threshold 0.21 is 0.6205213226489822\n",
      "F1 score at threshold 0.22 is 0.6249591789129928\n",
      "F1 score at threshold 0.23 is 0.6284173682716867\n",
      "F1 score at threshold 0.24 is 0.6322958131504484\n",
      "F1 score at threshold 0.25 is 0.6352713178294574\n",
      "F1 score at threshold 0.26 is 0.6373303278286861\n",
      "F1 score at threshold 0.27 is 0.6407102469993056\n",
      "F1 score at threshold 0.28 is 0.6427890644594934\n",
      "F1 score at threshold 0.29 is 0.6449620253164556\n",
      "F1 score at threshold 0.3 is 0.6457811621607809\n",
      "F1 score at threshold 0.31 is 0.646961894953656\n",
      "F1 score at threshold 0.32 is 0.6482665419200583\n",
      "F1 score at threshold 0.33 is 0.6484612960823443\n",
      "F1 score at threshold 0.34 is 0.6494484514212983\n",
      "F1 score at threshold 0.35 is 0.650677231115156\n",
      "F1 score at threshold 0.36 is 0.6528340627870536\n",
      "F1 score at threshold 0.37 is 0.6533216973928221\n",
      "F1 score at threshold 0.38 is 0.6535307391711156\n",
      "F1 score at threshold 0.39 is 0.6537756292715451\n",
      "F1 score at threshold 0.4 is 0.6538698995341528\n",
      "F1 score at threshold 0.41 is 0.6536585365853659\n",
      "F1 score at threshold 0.42 is 0.6543266475644698\n",
      "F1 score at threshold 0.43 is 0.6536364690871129\n",
      "F1 score at threshold 0.44 is 0.6530373147443033\n",
      "F1 score at threshold 0.45 is 0.6522844139724571\n",
      "F1 score at threshold 0.46 is 0.649650307848646\n",
      "F1 score at threshold 0.47 is 0.6496191512513602\n",
      "F1 score at threshold 0.48 is 0.6486817152994434\n",
      "F1 score at threshold 0.49 is 0.6464358704403295\n",
      "F1 score at threshold 0.5 is 0.6449143502541256\n"
     ]
    }
   ],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get the test set predictions as well and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 489s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_noemb_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model building is done, it might be a good idea to clean up some memory before we go to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got some baseline GRU model without pre-trained embeddings. Now let us use the provided embeddings and rebuild the model again to see the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove Embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, let us use the Glove embeddings and rebuild the GRU model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [16:03, 2278.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('C:\\\\Users\\\\maryam\\\\Desktop\\\\Python Program\\\\Deep Learning\\\\2\\\\glove.840B.300d\\\\glove.840B.300d.txt',encoding=\"utf-8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \") # Return a list of the words in the string\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert values to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [04:44<00:00, 10.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = text[:-1].split()[:30]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][:3000])])\n",
    "val_y = np.array(val_df[\"target\"][:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size) # Return the ceiling of x as an Integral\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True),input_shape=(30, 300)))\n",
    "model.add(Bidirectional(GRU(64)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maryam\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1064s 1s/step - loss: 0.1348 - accuracy: 0.9491 - val_loss: 0.1145 - val_accuracy: 0.9507\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 673s 673ms/step - loss: 0.1163 - accuracy: 0.9547 - val_loss: 0.1086 - val_accuracy: 0.9573\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 655s 655ms/step - loss: 0.1156 - accuracy: 0.9548 - val_loss: 0.1095 - val_accuracy: 0.9567\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 653s 653ms/step - loss: 0.1128 - accuracy: 0.9557 - val_loss: 0.1021 - val_accuracy: 0.9617\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 671s 671ms/step - loss: 0.1113 - accuracy: 0.9561 - val_loss: 0.1037 - val_accuracy: 0.9597\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 674s 674ms/step - loss: 0.1075 - accuracy: 0.9574 - val_loss: 0.0998 - val_accuracy: 0.9600\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 651s 651ms/step - loss: 0.1063 - accuracy: 0.9582 - val_loss: 0.1039 - val_accuracy: 0.9587\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 594s 594ms/step - loss: 0.1078 - accuracy: 0.9576 - val_loss: 0.0979 - val_accuracy: 0.9610\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 511s 511ms/step - loss: 0.1069 - accuracy: 0.9578 - val_loss: 0.1003 - val_accuracy: 0.9610\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 363s 363ms/step - loss: 0.1011 - accuracy: 0.9596 - val_loss: 0.0983 - val_accuracy: 0.9600\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 352s 352ms/step - loss: 0.1004 - accuracy: 0.9599 - val_loss: 0.0979 - val_accuracy: 0.9630\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 346s 346ms/step - loss: 0.1027 - accuracy: 0.9594 - val_loss: 0.1005 - val_accuracy: 0.9570\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 345s 345ms/step - loss: 0.1005 - accuracy: 0.9603 - val_loss: 0.0994 - val_accuracy: 0.9583\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 355s 355ms/step - loss: 0.1015 - accuracy: 0.9595 - val_loss: 0.0998 - val_accuracy: 0.9600\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 357s 357ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.1010 - val_accuracy: 0.9583\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.0981 - accuracy: 0.9603 - val_loss: 0.1011 - val_accuracy: 0.9593\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 345s 345ms/step - loss: 0.1004 - accuracy: 0.9603 - val_loss: 0.1002 - val_accuracy: 0.9603\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.0994 - accuracy: 0.9598 - val_loss: 0.0988 - val_accuracy: 0.9593\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.0935 - accuracy: 0.9629 - val_loss: 0.0988 - val_accuracy: 0.9587\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.0904 - accuracy: 0.9632 - val_loss: 0.0991 - val_accuracy: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10f8134ecc8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=20,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1468it [15:54,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained embeddings seem to give better results comapred to non-pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
